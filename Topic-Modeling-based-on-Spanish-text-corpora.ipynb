{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling based on Spanish text corpora from the Hispanic Digital Library\n",
    "\n",
    "The [Hispanic Digital Library](http://www.bne.es/en/Catalogos/BibliotecaDigitalHispanica/Acercade/) is the digital library of the Biblioteca Nacional de España. It provides access to thousands of digitised documents, including books printed from the 15th to the 20th century, manuscripts, drawings, engravings, pamphlets, posters, photographs, maps, atlases, music scores, historic newspapers and magazines and audio recordings.\n",
    "\n",
    "This example is based on the works of the author Manuel José Quintana since the library provides his works openly available as OCR output text.\n",
    "\n",
    "Topic Models are a type of statistical language models used for discovering hidden structure in a collection of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the text\n",
    "The web interface allows to retrieve the OCR text of the documents. Each item provides a link to visualize the content from where the OCR output text can be downloaded. See, for example, the following [link](http://bdh-rd.bne.es/viewer.vm?id=0000131223&page=1).\n",
    "\n",
    "<img src=\"images/bdh.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text files have been stored in the folder [BNE](./BNE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the txt files\n",
    "\n",
    "The dataset comprises several files and formats. We have prepared the text files in this project to work with them.\n",
    "\n",
    "Note: the original dataset did not include a CSV file. It was generated from a Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into works\n",
    "works = pd.read_csv('BNE/bne.csv', encoding='utf8')\n",
    "\n",
    "# Print head\n",
    "works.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the files and extracting the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in works.iterrows():\n",
    "    \n",
    "    try:\n",
    "        file = \"BNE/\"+ row['file'];\n",
    "        f = open(file, \"r\")\n",
    "        text = f.read()\n",
    "        \n",
    "        works.loc[index, 'original_text'] = text\n",
    "                \n",
    "    except:\n",
    "        print(\"An exception occurred\", sys.exc_info()[0]) \n",
    "        works.loc[index, 'original_text'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing the content of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation/lower casing/stopwords\n",
    "\n",
    "Next, let’s perform a simple preprocessing on the content to make them more amenable for analysis, and reliable results. We use a regular expression to remove any punctuation, lowercase the text, remove stopwords and then remove non Spanish words since the OCR may have some errors.\n",
    "\n",
    "We use wordnet to verify if the word exists. We also have added some specific stopwords to enhance the performance.\n",
    "\n",
    "The initial_clean function performs an initial clean by removing punctuations, uppercase text, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_clean(text):\n",
    "    \"\"\"\n",
    "    Function to clean text-remove punctuations, lowercase text etc.    \n",
    "    \"\"\"\n",
    "   \n",
    "    text = text.lower() # lower case text\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use a language_detector for Spanish to remove non existent words. Due to the text provided in the dataset many words are not existent. While the result is better, the performance is reduced by removing non existent words.\n",
    "\n",
    "Below there is an example of how to identify the language of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "lang = translator.detect(\"La casa de Fernando es muy bonita\").lang\n",
    "print(lang, \":\", \"La casa de Fernando es muy bonita\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also lemmatize words to use their roots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "spanishstemmer=SnowballStemmer('spanish')\n",
    "\n",
    "print(spanishstemmer.stem(\"habían\"))\n",
    "print(spanishstemmer.stem(\"campo\"))\n",
    "print(spanishstemmer.stem(\"casa\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function could be improved using additional filters such as language identification and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(text):\n",
    "    filtered_text = [] \n",
    "    \n",
    "    for token in text:\n",
    "\n",
    "        if len(token) <= 2:\n",
    "            continue\n",
    "        else:\n",
    "            filtered_text.append(token)\n",
    "            \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words\n",
    "\n",
    "Stop words are words which does not add much meaning to a sentence. For example, the words in English like the, he, have, etc.\n",
    "\n",
    "There are several Python packages that provide stopwords lists and they can also be customized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('spanish')\n",
    "stop_words.extend(['habia', 'quo', 'dió', 'algún','darién', 'dia', 'sing', 'babia', 'habian', 'despues', 'indic', 'ele', 'sólo', 'según', 'jos', 'jucef', 'pers', 'the', 'ra.', '.—núm', 'aben'])\n",
    "def remove_stop_words(text):\n",
    "     return [word for word in text if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function to perform the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_all(text):\n",
    "    \"\"\"\n",
    "    This function applies all the functions above into one\n",
    "    \"\"\"\n",
    "    return remove_stop_words(remove_words(initial_clean(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we process the original text by using the function apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean reviews and create new column \"tokenized\" \n",
    "import time   \n",
    "t1 = time.time()   \n",
    "works['tokenized_text'] = works['original_text'].apply(apply_all)    \n",
    "t2 = time.time()  \n",
    "print(\"Time to clean and tokenize\", len(works), \"reviews:\", (t2-t1)/60, \"min\") #Time to clean and tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works['tokenized_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gensim Dictionary and Corpus\n",
    "Topic modeling using LDA are based on the dictionary and the corpus. This example is based on gensim library for building both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = works['tokenized_text']\n",
    "\n",
    "#Creating term dictionary of corpus, where each unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(tokenized)\n",
    "#Filter terms which occurs in less than 1 document and more than 80% of the documents.\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "#convert the dictionary to a bag of words corpus \n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in tokenized]\n",
    "#print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Topic Model\n",
    "In this step, num_topics is the number of topics to be created and passes corresponds to the number of times to iterate through the entire corpus. By running the LDA algorithm we get the topics as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "#LDA\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 5, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model_combined.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows the 5 topics created and the 4 words within each topic which best describes them. From the above output we could guess that each topic and their corresponding words revolve around a common theme (For e.g., topic 3 is related to franceses and cortes)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

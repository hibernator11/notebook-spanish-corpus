{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building language models based on Spanish text corpora\n",
    "\n",
    "This example is based on the dataset that provides text in computer readable format of the 2 books of the volume dedicated to the province of Avila of the Monumental Catalogue of Spain written by Manuel Gómez-Moreno (1900-1901). The dataset is available at [figshare](https://figshare.com/articles/Transcripci_n_del_Cat_logo_Monumental_de_Espa_a_Provincia_de_vila_por_Manuel_G_mez_Moreno_1900-1901_/12006318). \n",
    "\n",
    "After automatic transcription based on Transkribus, the text was manually revised. The transcriptions were carried out by Raquel Liceras-Garrido, Alba Comino and Patricia Murrieta-Flores under the project “Goodbye reading glasses: a Machine Learning experiment on handwriting documents”, funded by the Faculty of Arts and Social Sciences and the Digital Humanities Hub of Lancaster University (UK).\n",
    "\n",
    "The project produced several datasets based on other Spanish cities including [Soria](https://figshare.com/articles/Transcripci_n_del_Cat_logo_Monumental_de_la_Provincia_de_Soria_por_Juan_Cabr_1916-1917_/12006273\n",
    ") and [Burgos](https://figshare.com/articles/Transcripci_n_del_Cat_logo_Monumental_y_Art_stico_de_la_Provincia_de_Burgos_por_Narciso_Sentenach_1925_/12006327).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the txt files\n",
    "\n",
    "The dataset comprises several files and formats. We have prepared the text files in this project to work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path('CME_Avila/GM_Avila_v1_Text2_Procesado.txt')\n",
    "\n",
    "text = ''\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    with open(filename, 'r') as myfile:\n",
    "        text = myfile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We get the text from the second file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path('CME_Avila/GM_Avila_v1_Text_Procesado_51-258.txt')\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    with open(filename, 'r') as myfile:\n",
    "        text += myfile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words\n",
    "\n",
    "Stop words are words which does not add much meaning to a sentence. For example, the words in English like the, he, have, etc.\n",
    "\n",
    "There are several Python packages that provide stopwords lists and they can also be customized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding specific stopwords\n",
    "customized_stop_words = [\"que\", \"es\", \"un\", \"una\", \"do\", \"toda\", \"hacia\"] + stopwords.words('spanish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(stopwords = customized_stop_words, collocations=False, background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(text)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down a text paragraph into smaller chunks such as words is called Tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization\n",
    "Sentence tokenizer breaks text paragraph into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text=sent_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "tokenized_word=word_tokenize(text)\n",
    "stop = stopwords.words('spanish') + list(string.punctuation)\n",
    "cleaned_text = [i for i in word_tokenize(text.lower()) if i not in stop]\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(cleaned_text)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution Plot\n",
    "import matplotlib.pyplot as plt\n",
    "fdist.plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"spanish\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams are consecutive words in a sentence. Let's see how to generate them from a sentence in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence = \"Ojos de buey achaflanados y una ventana de arco agudo, prestaban la mayor cantidad de luz al interior, puesto que á los costados solo había saeteras\"\n",
    "\n",
    "print(first_sentence) \n",
    " \n",
    "# Get the bigrams\n",
    "print(list(bigrams(word_tokenize(first_sentence))))\n",
    " \n",
    "# Get the trigrams\n",
    "print (list(trigrams(word_tokenize(first_sentence))))\n",
    " \n",
    "# Get the padded trigrams\n",
    "print (list(trigrams(word_tokenize(first_sentence), pad_left=True, pad_right=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    " \n",
    "for sentence in sent_tokenize(text):\n",
    "    for w1, w2, w3 in trigrams(word_tokenize(sentence), pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    "\n",
    "print(model[\"El\", \"Barco\"][\"de\"])\n",
    "print(model[\"Fragmentos\", \"de\"][\"retablo\"])\n",
    "\n",
    "#print model[\"what\", \"the\"][\"economists\"] # \"economists\" follows \"what the\" 2 times\n",
    "#print model[\"what\", \"the\"][\"nonexistingword\"] # 0 times\n",
    "#print model[None, None][\"The\"] # 8839 sentences start with \"The\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's transform the counts to probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    \n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count\n",
    " \n",
    "print(model[\"El\", \"Barco\"][\"de\"])\n",
    "print(model[\"Fragmentos\", \"de\"][\"retablo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have a trigram language model, let’s generate some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = [None, None]\n",
    " \n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "    r = random.random()\n",
    "    accumulator = .0\n",
    "    \n",
    "    print(txt[-2:])\n",
    " \n",
    "    for word in model[tuple(txt[-2:])].keys():\n",
    "        accumulator += model[tuple(txt[-2:])][word]\n",
    " \n",
    "        if accumulator >= r:\n",
    "            txt.append(word)\n",
    "            break\n",
    " \n",
    "    if txt[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    " \n",
    "print (' '.join([t for t in txt if t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Liceras-Garrido, Raquel; Comino, Alba; Murrieta-Flores, Patricia (2020): Transcripción del Catálogo Monumental de España: Provincia de Ávila por Manuel Gómez Moreno (1900-1901). figshare. Dataset. https://doi.org/10.6084/m9.figshare.12006318.v1\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk\n",
    "\n",
    "https://nlpforhackers.io/language-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

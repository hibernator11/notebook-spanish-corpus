{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library of Congress & Chronicling America\n",
    "\n",
    "This notebook uses historic newspapers and select digitized newspaper pages provided by [Chronicling America](https://chroniclingamerica.loc.gov/about/) (ISSN 2475-2703).\n",
    "\n",
    "This example is based on the [*About Hispano América*](https://chroniclingamerica.loc.gov/lccn/sn87021178/) that was published in San Francisco.\n",
    "\n",
    "[Chronicling America](https://chroniclingamerica.loc.gov/about/api/) provides an extensive application programming interface (API) which you can use to explore all of the data. The information is also [published as JSON](https://chroniclingamerica.loc.gov/lccn/sn87021178.json), including the OCR text files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from numpy import mean, ones\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's retrieve the results!\n",
    "\n",
    "The *About Hispano América* is accessible via a JSON file including all metadata. The attribute *issues* contains the URLs of all issues that, in turn, contain all *pages*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://chroniclingamerica.loc.gov/lccn/sn87021178.json'\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "ca_dict = json.loads(r.text)\n",
    "\n",
    "df = pd.DataFrame(ca_dict['issues'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the OCR texts from Chronicling America\n",
    "\n",
    "**Note:**  This step may take a while to process due to the number of issues. Uncomment the code in order to execute this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(index, row['url'])\n",
    "    response = requests.get(row['url'])\n",
    "    print(response)\n",
    "    text = ''\n",
    "    if response:\n",
    "        item = json.loads(response.text)\n",
    "        text = ''\n",
    "        for p in item['pages']:\n",
    "            res_page = requests.get(p['url']) \n",
    "            json_page = json.loads(res_page.text)\n",
    "            print(\"text\"+json_page['text'])\n",
    "            \n",
    "            text = text + requests.get(json_page['text']).text.replace('\\n','').encode('latin1').decode('utf8')\n",
    "        \n",
    "        outF = open('lc-editions/{}'.format(row['url'].replace('https://chroniclingamerica.loc.gov/lccn/sn87021178/','').replace('/', '_').replace('json', 'txt')), \"w\")\n",
    "        outF.write(text) \n",
    "        outF.close()\n",
    "        \n",
    "df.head(10)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we load the text into pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(index, row['url'])\n",
    "   \n",
    "    filename = Path('lc-editions/{}'.format(row['url'].replace('https://chroniclingamerica.loc.gov/lccn/sn87021178/','').replace('/', '_').replace('json', 'txt')))\n",
    "    \n",
    "    text = ''\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r') as myfile:\n",
    "            text = myfile.read()\n",
    "  \n",
    "    df.loc[index, 'ocr_text'] = text\n",
    "\n",
    "df.head(10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the years from the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in df.iterrows():\n",
    "    \n",
    "    try:\n",
    "        df.loc[index, 'year'] = int(row['date_issued'][:4])\n",
    "    except:\n",
    "        df.loc[index, 'year'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation/lower casing/stopwords\n",
    "Next, let’s perform a simple preprocessing on the content to make them more amenable for analysis, and reliable results. We use a regular expression to remove any punctuation, lowercase the text, remove stopwords and then remove non Spanish words since the OCR may have some errors.\n",
    "\n",
    "We use wordnet to verify if the word exists. We also have added some specific stopwords to enhance the performance.\n",
    "\n",
    "The initial_clean function performs an initial clean by removing punctuations, uppercase text, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_clean(text):\n",
    "    \"\"\"\n",
    "    Function to clean text-remove punctuations, lowercase text etc.    \n",
    "    \"\"\"\n",
    "    regex = re.compile('[\\\",\\.!?]')\n",
    "    regex.sub('', text)\n",
    "    text = text.lower() # lower case text\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(text):\n",
    "    filtered_text = [] \n",
    "    \n",
    "    for token in text:\n",
    "\n",
    "        if len(token) <= 2:\n",
    "            continue\n",
    "        else:\n",
    "            filtered_text.append(token)\n",
    "            \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words\n",
    "\n",
    "Stop words are words which does not add much meaning to a sentence. For example, the words in English like the, he, have, etc.\n",
    "\n",
    "There are several Python packages that provide stopwords lists and they can also be customized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('spanish')\n",
    "stop_words.extend(['with', 'song', 'guitar', 'spanish', 'typical'])\n",
    "def remove_stop_words(text):\n",
    "     return [word for word in text if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function to perform the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_all(text):\n",
    "    \"\"\"\n",
    "    This function applies all the functions above into one\n",
    "    \"\"\"\n",
    "    return remove_stop_words(remove_words(initial_clean(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we process the original text by using the function apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean reviews and create new column \"tokenized\" \n",
    "import time   \n",
    "t1 = time.time()   \n",
    "df['tokenized_text'] = df['ocr_text'].apply(apply_all)    \n",
    "t2 = time.time()  \n",
    "print(\"Time to clean and tokenize\", len(df), \"reviews:\", (t2-t1)/60, \"min\") #Time to clean and tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gensim Dictionary and Corpus\n",
    "Topic modeling using LDA are based on the dictionary and the corpus. This example is based on gensim library for building both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = df['tokenized_text']\n",
    "\n",
    "#Creating term dictionary of corpus, where each unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(tokenized)\n",
    "#Filter terms which occurs in less than 1 document and more than 80% of the documents.\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "#convert the dictionary to a bag of words corpus \n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in tokenized]\n",
    "#print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Topic Model\n",
    "In this step, num_topics is the number of topics to be created and passes corresponds to the number of times to iterate through the entire corpus. By running the LDA algorithm we get the topics as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "#LDA\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 5, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model_combined.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows the 5 topics created and the 4 words within each topic which best describes them. From the above output we could guess that each topic and their corresponding words revolve around a common theme (For e.g., topic 3 is related to independencia and trabajadores)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
